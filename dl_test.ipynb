{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar\n",
    "from pypots.data import load_specific_dataset\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.utils.metrics import calc_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_memory)\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/torch/cuda/__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_properties(0).total_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 14:15:24 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\n",
      "2024-01-25 14:15:24 [INFO]: Starting preprocessing physionet_2012...\n",
      "2024-01-25 14:15:24 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2024-01-25 14:15:24 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2024-01-25 14:15:24 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2024-01-25 14:15:24 [INFO]: Loaded successfully!\n",
      "2024-01-25 14:15:47 [INFO]: Using the given device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11988, 48, 37)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "You are trying to use CUDA for model training, but CUDA is not available in your environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (11988, 48, 37), 11988 samples, 48 time steps, 37 features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Model training. This is PyPOTS showtime.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m saits \u001b[38;5;241m=\u001b[39m \u001b[43mSAITS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m37\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_inner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_v\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\u001b[39;00m\n\u001b[1;32m     16\u001b[0m saits\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/pypots/imputation/saits/model.py:160\u001b[0m, in \u001b[0;36mSAITS.__init__\u001b[0;34m(self, n_steps, n_features, n_layers, d_model, d_inner, n_heads, d_k, d_v, dropout, attn_dropout, diagonal_attention_mask, ORT_weight, MIT_weight, batch_size, epochs, patience, customized_loss_func, optimizer, num_workers, device, saving_path, model_saving_strategy)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     n_steps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     model_saving_strategy: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    159\u001b[0m ):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_saving_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d_model \u001b[38;5;241m!=\u001b[39m n_heads \u001b[38;5;241m*\u001b[39m d_k:\n\u001b[1;32m    171\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‼️ d_model must = n_heads * d_k, it should be divisible by n_heads \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the result should be equal to d_k, but got d_model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_heads=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, d_k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/pypots/imputation/base.py:194\u001b[0m, in \u001b[0;36mBaseNNImputer.__init__\u001b[0;34m(self, batch_size, epochs, patience, num_workers, device, saving_path, model_saving_strategy)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    186\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     model_saving_strategy: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    193\u001b[0m ):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_saving_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/pypots/base.py:522\u001b[0m, in \u001b[0;36mBaseNNModel.__init__\u001b[0;34m(self, batch_size, epochs, patience, num_workers, device, saving_path, model_saving_strategy)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    514\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     model_saving_strategy: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    521\u001b[0m ):\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaving_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_saving_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m patience \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# early stopping on patience won't work if it is set as < 0\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/pypots/base.py:81\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, device, saving_path, model_saving_strategy)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# set up the device for model running below\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# set up saving_path to save the trained model and training logs\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_path(saving_path)\n",
      "File \u001b[0;32m~/miniforge3/envs/aquifer-torch/lib/python3.10/site-packages/pypots/base.py:143\u001b[0m, in \u001b[0;36mBaseModel._setup_device\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# check CUDA availability if using CUDA\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 143\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    144\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to use CUDA for model training, but CUDA is not available in your environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: You are trying to use CUDA for model training, but CUDA is not available in your environment."
     ]
    }
   ],
   "source": [
    "\n",
    "# Data preprocessing. Tedious, but PyPOTS can help.\n",
    "data = load_specific_dataset('physionet_2012')  # PyPOTS will automatically download and extract it.\n",
    "X = data['X']\n",
    "num_samples = len(X['RecordID'].unique())\n",
    "X = X.drop(['RecordID', 'Time'], axis = 1)\n",
    "X = StandardScaler().fit_transform(X.to_numpy())\n",
    "X = X.reshape(num_samples, 48, -1)\n",
    "X_ori = X  # keep X_ori for validation\n",
    "X = mcar(X, 0.1)  # randomly hold out 10% observed values as ground truth\n",
    "dataset = {\"X\": X}  # X for model input\n",
    "print(X.shape)  # (11988, 48, 37), 11988 samples, 48 time steps, 37 features\n",
    "\n",
    "# Model training. This is PyPOTS showtime.\n",
    "saits = SAITS(n_steps=48, n_features=37, n_layers=2, d_model=256, d_inner=128, n_heads=4, d_k=64, d_v=64, dropout=0.1, epochs=10, device='cuda')\n",
    "# Here I use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "saits.fit(dataset)\n",
    "imputation = saits.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X':         RecordID  Time  ALP  ALT  AST  Albumin  BUN  Bilirubin  Cholesterol  \\\n",
       " 0         132539   0.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 1         132539   1.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 2         132539   2.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 3         132539   3.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 4         132539   4.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " ...          ...   ...  ...  ...  ...      ...  ...        ...          ...   \n",
       " 575419    163037  43.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 575420    163037  44.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 575421    163037  45.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 575422    163037  46.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " 575423    163037  47.0  NaN  NaN  NaN      NaN  NaN        NaN          NaN   \n",
       " \n",
       "         Creatinine  ...  RespRate  SaO2  SysABP   Temp  TroponinI  TroponinT  \\\n",
       " 0              NaN  ...      19.0   NaN     NaN  35.35        NaN        NaN   \n",
       " 1              NaN  ...      18.0   NaN     NaN    NaN        NaN        NaN   \n",
       " 2              NaN  ...      19.0   NaN     NaN    NaN        NaN        NaN   \n",
       " 3              NaN  ...      20.0   NaN     NaN  37.80        NaN        NaN   \n",
       " 4              NaN  ...      20.0   NaN     NaN    NaN        NaN        NaN   \n",
       " ...            ...  ...       ...   ...     ...    ...        ...        ...   \n",
       " 575419         NaN  ...       NaN   NaN   122.0    NaN        NaN        NaN   \n",
       " 575420         NaN  ...       NaN   NaN     NaN    NaN        NaN        NaN   \n",
       " 575421         NaN  ...       NaN   NaN   147.0  37.30        NaN        NaN   \n",
       " 575422         NaN  ...       NaN   NaN    90.0    NaN        NaN        NaN   \n",
       " 575423         NaN  ...       NaN   NaN    99.0    NaN        NaN        NaN   \n",
       " \n",
       "         Urine  WBC  Weight  pH  \n",
       " 0       480.0  NaN    -1.0 NaN  \n",
       " 1        30.0  NaN     NaN NaN  \n",
       " 2       170.0  NaN     NaN NaN  \n",
       " 3        60.0  NaN     NaN NaN  \n",
       " 4         NaN  NaN     NaN NaN  \n",
       " ...       ...  ...     ...  ..  \n",
       " 575419   60.0  NaN     NaN NaN  \n",
       " 575420    NaN  NaN     NaN NaN  \n",
       " 575421   50.0  NaN     NaN NaN  \n",
       " 575422    NaN  NaN     NaN NaN  \n",
       " 575423   30.0  NaN     NaN NaN  \n",
       " \n",
       " [575424 rows x 39 columns],\n",
       " 'y':           In-hospital_death\n",
       " RecordID                   \n",
       " 139169                    1\n",
       " 139644                    0\n",
       " 139068                    0\n",
       " 136040                    0\n",
       " 136292                    0\n",
       " ...                     ...\n",
       " 162054                    0\n",
       " 154077                    0\n",
       " 156131                    0\n",
       " 155844                    0\n",
       " 159365                    0\n",
       " \n",
       " [11988 rows x 1 columns],\n",
       " 'ICUType':           ICUType\n",
       " RecordID         \n",
       " 132539        4.0\n",
       " 132540        2.0\n",
       " 132541        3.0\n",
       " 132543        3.0\n",
       " 132545        3.0\n",
       " ...           ...\n",
       " 163029        4.0\n",
       " 163033        3.0\n",
       " 163034        4.0\n",
       " 163035        1.0\n",
       " 163037        1.0\n",
       " \n",
       " [11988 rows x 1 columns]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
